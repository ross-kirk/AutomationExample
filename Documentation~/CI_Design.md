# CI Pipelines - High Devel Design 

## Overview
Running the runtime tests from this project in CI pipelines would look largely similar to how the builds themselves are invoked for most Unity CI. There are a few ways to tackle this. The quickest & easiest way would be to run the runtime tests in PlayMode within the editor, parallel to the build jobs within the same CI environment using the same runners. If running on the same system/platform as the builds it will usually be able to run with enough processing power to run the play mode editor, meaning tests can be quickly verified similarly to how you would do on a local machine. This can occasionally be risky if you're low on runners & want multiple build jobs though..

If there was budget for tests to run on a platform runner for the target build itself, such as StandaloneWin64, iOS or Android, you can run a custom build with tests deployed within the build, deploy the artifact to a real hardware device farm & run the tests from CLI, then gather the results, through things like ADB & XCode. This would allow you to pull results from the device & upload them to wherever test results are viewed by QA/Developers. This would be set up through a couple of runners/schedulers to spit out builds & then handle the interop with the device.

### Strategy - Simple Build Server Tests
I'd initially go with the former option for quick & easy tests being run during build steps. Create (or grab existing) docker image with the intended version of Unity for the project (6000.0.40f1), as well as anything required to run the build. Running in batchmode using the CLI options, specifically for Unity to [invoke the test runner](https://docs.unity3d.com/6000.2/Documentation/Manual/test-framework/run-tests-from-command-line.html). These jobs should ideally have any prep/setup of the build done before tests, and clean up after themselves to avoid any issues with builds lingering on next run. If there is a build distribution service in place, test results could be piped through to here as well.

Tests should be run in a separate scheduled job to the build, but should be available to run sequentially & parallel if required (if we want the build to be created no matter if tests pass, run parallel, if not, run sequentially before the build & fail early on build jobs). Tests could also be filtered/categorized using the NUnit test categories, then parallelised up into even more granular tests, such as Smoke & BVT depending on scope of the tests. That means on a merge request, you schedule smoke tests to run & pass before merge is allowed, nightly tests could run ALL tests, and on ad-hoc builds you might just want to run a Build Verification Suite to know the build launches & doesn't crash (most of the time). These can be split into separate jobs & run depending on need of the CI pipeline.

If there are unit & integration tests, such as the editor tests or any tests that don't rely specifically on Unity, these could also be split off and run in separate scheduled jobs in parallel on less powered runners to save on processing, and depending on severity of the tests failing, you can fully fail build or block merge, which also stops the Runtime tests from starting. 

Ad-hoc runs of tests should be possible too, so QA/Devs/anyone can just spin up a test run in the CI.

Finally, a separate job runs the test result gathering for the NUnit xml results which should be kept for X amount of time after a job to view, and I'd like to use an HTML converter to pipe out the results into a readable results file, so this can be uploaded to a shared viewing space (internal test automation viewing site or something). These should be linked to the builds, so tracking between build & result is as simple & easy as possible, as well as seeing where things might have been broken & tracing the root cause of failures.

### Strategy - Player Tests
If the budget allowed, we can build a player build with tests included, & upload that to device farm (ios/android) or an automation runner for windows/mac etc, then run the tests through the CLI to this using things like ADB/XCode/terminal. The results XML from these tests can be saved to the persistent data path & pulled off the device to upload to the earlier mentioned results store. This gives extra peace of mind that tests are being carried out on the bare metal & platforms we're shipping to. It also allows you to test more environments (OS/hardware configs), because the runners tend to stay the same, becoming stale/reliable. Different memory configs might give different results.